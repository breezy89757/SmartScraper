"""
SmartScraper - AI é©…å‹•çš„çˆ¬èŸ²ç”Ÿæˆå™¨
FastAPI ä¸»å…¥å£
"""
import os
from contextlib import asynccontextmanager
from typing import Optional
from pathlib import Path

from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv

from browser.playwright_client import PlaywrightClient
from agents.analyzer import PageAnalyzer
from agents.generator import ScraperGenerator
from sandbox.executor import SandboxExecutor

load_dotenv()

# å…¨åŸŸå®¢æˆ¶ç«¯
browser_client: Optional[PlaywrightClient] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç†"""
    global browser_client
    browser_client = PlaywrightClient()
    await browser_client.start()
    print("âœ… Playwright ç€è¦½å™¨å·²å•Ÿå‹•")
    yield
    await browser_client.stop()
    print("ğŸ›‘ ç€è¦½å™¨å·²é—œé–‰")


app = FastAPI(
    title="SmartScraper",
    description="AI é©…å‹•çš„çˆ¬èŸ²ç”Ÿæˆå™¨ - è¼¸å…¥ URL + ç›®æ¨™ï¼Œè‡ªå‹•ç”¢ç”Ÿçˆ¬èŸ²ç¨‹å¼ç¢¼",
    version="0.1.0",
    lifespan=lifespan
)

# éœæ…‹æª”æ¡ˆ
static_path = Path(__file__).parent / "static"
if static_path.exists():
    app.mount("/static", StaticFiles(directory=str(static_path)), name="static")


@app.get("/", response_class=HTMLResponse)
async def index():
    """é¦–é  - ç¶²é ä»‹é¢"""
    index_file = static_path / "index.html"
    if index_file.exists():
        return HTMLResponse(index_file.read_text(encoding="utf-8"))
    return HTMLResponse("<h1>SmartScraper</h1><p>Static files not found</p>")


# ===== Request/Response Models =====

class AnalyzeRequest(BaseModel):
    url: str
    goal: str  # ä¾‹å¦‚ "æŠ“å–å•†å“åƒ¹æ ¼"


class GenerateRequest(BaseModel):
    url: str
    goal: str
    use_vision: bool = True


class ExecuteRequest(BaseModel):
    code: str
    url: str


class FullPipelineRequest(BaseModel):
    url: str
    goal: str
    use_vision: bool = True
    auto_execute: bool = True


# ===== Endpoints =====

@app.get("/health")
async def health():
    return {"status": "ok", "browser": browser_client is not None}


@app.post("/analyze")
async def analyze_page(request: AnalyzeRequest):
    """
    åˆ†æç¶²é çµæ§‹
    
    è¿”å›ï¼šå»ºè­°çš„ selectors å’Œè³‡æ–™çµæ§‹
    """
    if not browser_client:
        raise HTTPException(500, "ç€è¦½å™¨æœªå•Ÿå‹•")
    
    # è¼‰å…¥ä¸¦åˆ†æç¶²é 
    page_data = await browser_client.analyze_page(request.url)
    
    # AI åˆ†æ
    analyzer = PageAnalyzer()
    try:
        result = await analyzer.analyze(
            user_goal=request.goal,
            page_title=page_data.title,
            simplified_html=page_data.simplified_html,
            screenshot_base64=page_data.screenshot_base64
        )
        
        return {
            "page_title": page_data.title,
            "analysis": {
                "target": result.target_description,
                "selectors": result.suggested_selectors,
                "structure": result.data_structure,
                "page_type": result.page_type
            }
        }
    finally:
        await analyzer.close()


@app.post("/generate")
async def generate_scraper(request: GenerateRequest):
    """
    ç”Ÿæˆçˆ¬èŸ²ç¨‹å¼ç¢¼
    
    å®Œæ•´æµç¨‹ï¼šåˆ†æ â†’ ç”Ÿæˆç¨‹å¼ç¢¼
    """
    if not browser_client:
        raise HTTPException(500, "ç€è¦½å™¨æœªå•Ÿå‹•")
    
    # Step 1: è¼‰å…¥ç¶²é 
    page_data = await browser_client.analyze_page(request.url)
    
    # Step 2: åˆ†æ
    analyzer = PageAnalyzer()
    try:
        analysis = await analyzer.analyze(
            user_goal=request.goal,
            page_title=page_data.title,
            simplified_html=page_data.simplified_html,
            screenshot_base64=page_data.screenshot_base64 if request.use_vision else None
        )
    finally:
        await analyzer.close()
    
    # [Debug] é¡¯ç¤ºåˆ†æçµæœ
    print("\n" + "="*50)
    print("ğŸ¤– Analyzer æ€è€ƒçµæœ (å‚³çµ¦ Generator çš„è¦æ ¼æ›¸):")
    print("-" * 50)
    print(f"ğŸ“Œ ç›®æ¨™æè¿°: {analysis.target_description}")
    print(f"ğŸ” å»ºè­° Selectors: {analysis.suggested_selectors}")
    print(f"ğŸ“ é æœŸè³‡æ–™çµæ§‹: {analysis.data_structure}")
    print(f"ğŸ“„ é é¢é¡å‹: {analysis.page_type}")
    print("="*50 + "\n")
    
    # Step 3: ç”Ÿæˆç¨‹å¼ç¢¼
    generator = ScraperGenerator()
    try:
        code_result = await generator.generate(
            url=request.url,
            target_description=analysis.target_description,
            selectors=analysis.suggested_selectors,
            data_structure=analysis.data_structure,
            page_type=analysis.page_type
        )
        
        return {
            "analysis": {
                "target": analysis.target_description,
                "selectors": analysis.suggested_selectors,
                "structure": analysis.data_structure
            },
            "generated_code": code_result.code,
            "imports": code_result.imports,
            "explanation": code_result.explanation
        }
    finally:
        await generator.close()


@app.post("/execute")
async def execute_code(request: ExecuteRequest):
    """
    åœ¨æ²™ç®±ä¸­åŸ·è¡Œç¨‹å¼ç¢¼
    
    âš ï¸ åªåŸ·è¡Œå—ä¿¡ä»»çš„ç¨‹å¼ç¢¼
    """
    executor = SandboxExecutor()
    result = executor.execute(request.code, request.url)
    
    if result.success:
        return {
            "success": True,
            "data": result.data,
            "stdout": result.stdout
        }
    else:
        return JSONResponse(
            status_code=400,
            content={
                "success": False,
                "error": result.error,
                "stdout": result.stdout
            }
        )


class FixRequest(BaseModel):
    original_code: str
    url: str
    goal: str
    execution_result: str


@app.post("/fix")
async def fix_code(request: FixRequest):
    """
    AI ä¿®æ­£ç¨‹å¼ç¢¼

    æ ¹æ“šåŸ·è¡Œçµæœä¿®æ­£çˆ¬èŸ²ç¨‹å¼ç¢¼
    """
    from agents.openai_client import AzureOpenAIClient
    import os

    deployment = os.getenv("AZURE_OPENAI_CODEX_DEPLOYMENT", "gpt-5.1-codex-max")
    client = AzureOpenAIClient(deployment=deployment)

    system_prompt = """ä½ æ˜¯ä¸€å€‹ Python çˆ¬èŸ²å°ˆå®¶ã€‚ä½¿ç”¨è€…çš„çˆ¬èŸ²ç¨‹å¼ç¢¼åŸ·è¡Œå¾Œè¿”å›ç©ºçµæœæˆ–éŒ¯èª¤ã€‚
è«‹åˆ†æå•é¡Œä¸¦ä¿®æ­£ç¨‹å¼ç¢¼ã€‚

è¦å‰‡ï¼š
1. ä¿æŒ scrape(url) å‡½æ•¸çµæ§‹
2. ä¿®æ­£ CSS selector æˆ–è³‡æ–™æå–é‚è¼¯
3. åªè¼¸å‡ºä¿®æ­£å¾Œçš„å®Œæ•´ç¨‹å¼ç¢¼ï¼Œä¸è¦è§£é‡‹
4. ä½¿ç”¨ requests + BeautifulSoup"""

    user_prompt = f"""ç›®æ¨™ç¶²å€: {request.url}
ä½¿ç”¨è€…ç›®æ¨™: {request.goal}

åŸå§‹ç¨‹å¼ç¢¼:
```python
{request.original_code}
```

åŸ·è¡Œçµæœ:
{request.execution_result}

è«‹æ±‚:
è«‹æ ¹æ“šä¸Šè¿°åŸ·è¡Œçµæœä¿®æ­£ç¨‹å¼ç¢¼ã€‚
1. å¦‚æœæ˜¯çˆ¬å–å¤±æ•— (ç©ºçµæœ/Null)ï¼Œè«‹å˜—è©¦æª¢æŸ¥ CSS Selector æˆ– HTML çµæ§‹ (å¯å˜—è©¦å°‹æ‰¾ä¸åŒç‰¹å¾µ)ã€‚
2. å¦‚æœæ˜¯åŸ·è¡ŒéŒ¯èª¤ (Exception)ï¼Œè«‹ä¿®æ­£èªæ³•æˆ–é‚è¼¯éŒ¯èª¤ã€‚
3. ç¢ºä¿ç¨‹å¼ç¢¼å¯ä»¥åœ¨å—é™æ²™ç®±ä¸­åŸ·è¡Œ (ä½¿ç”¨ requests, bs4, é¿å… os/sys)ã€‚"""

    try:
        response = await client.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.2
        )

        # æå–ç¨‹å¼ç¢¼
        content = response.content.strip()
        import re
        code_match = re.search(r'```python\s*(.*?)\s*```', content, re.DOTALL)
        if code_match:
            fixed_code = code_match.group(1)
        else:
            fixed_code = content

        return {"fixed_code": fixed_code}

    finally:
        await client.close()


@app.post("/full")
async def full_pipeline(request: FullPipelineRequest):
    """
    å®Œæ•´æµç¨‹ï¼šåˆ†æ â†’ ç”Ÿæˆ â†’ åŸ·è¡Œ
    
    ä¸€éµå®Œæˆçˆ¬èŸ²ä»»å‹™
    """
    if not browser_client:
        raise HTTPException(500, "ç€è¦½å™¨æœªå•Ÿå‹•")
    
    result = {
        "url": request.url,
        "goal": request.goal,
        "steps": {}
    }
    
    # Step 1: è¼‰å…¥ç¶²é 
    page_data = await browser_client.analyze_page(request.url)
    result["page_title"] = page_data.title
    
    # Step 2: åˆ†æ
    analyzer = PageAnalyzer()
    try:
        analysis = await analyzer.analyze(
            user_goal=request.goal,
            page_title=page_data.title,
            simplified_html=page_data.simplified_html,
            screenshot_base64=page_data.screenshot_base64 if request.use_vision else None
        )
        result["steps"]["analysis"] = {
            "target": analysis.target_description,
            "selectors": analysis.suggested_selectors,
            "structure": analysis.data_structure
        }
    finally:
        await analyzer.close()
    
    # Step 3: ç”Ÿæˆç¨‹å¼ç¢¼
    generator = ScraperGenerator()
    try:
        code_result = await generator.generate(
            url=request.url,
            target_description=analysis.target_description,
            selectors=analysis.suggested_selectors,
            data_structure=analysis.data_structure,
            page_type=analysis.page_type
        )
        result["steps"]["generation"] = {
            "code": code_result.code,
            "explanation": code_result.explanation
        }
    finally:
        await generator.close()
    
    # Step 4: åŸ·è¡Œ (å¦‚æœå•Ÿç”¨)
    if request.auto_execute:
        executor = SandboxExecutor()
        exec_result = executor.execute(code_result.code, request.url)
        result["steps"]["execution"] = {
            "success": exec_result.success,
            "data": exec_result.data if exec_result.success else None,
            "error": exec_result.error if not exec_result.success else None
        }
    
    return result


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8081)
